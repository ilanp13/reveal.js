<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Spectral Clustering</title>

		<meta name="description" content="A presentation on spectral clustering for applied mathematics seminar, May 2016">
		<meta name="author" content="Ilan Peretz">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/simple.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	</head>

	<body>
		<span style="position:fixed; top:0; right:0; z-index: 9999999;">בס"ד</span>
		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h2>Spectral Clustering</h2>
					<h3>By Ilan Peretz</h3>
				</section>

				<section>
					<section>
						<h2>What does "Spectral Clustering" means anyway?</h2>
					</section>

					<section>
						<h3>What is Clustering?</h3>
						<ul class="fragment">
							<li>
								<q>&ldquo;Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).&rdquo;</q><p>(-- <a href="https://en.wikipedia.org/wiki/Cluster_analysis" target="_blank">Wikipedia</a>)</p>
							</li>
							<li class="fragment">
								<p>So the goal is the same as we've seen before in this room: creating an algorithm that upon receiving a sample of sort, will be able to decide and know in which group to put that sample.</p>
							</li>
						</ul>
					</section>
					<section>
						<h3>What is the meaning of Spectral?</h3>
						<ul>
							<li class="fragment">
								<p>Spectral is coming from the terminology <b>Spectral Graph Theory</b>.</p>
							</li>
							<li class="fragment">
								<span><q>&ldquo;In mathematics, <b>spectral graph theory</b> is the study of properties of a graph in relationship to the characteristic polynomial, eigenvalues, and eigenvectors of matrices associated to the graph, such as its adjacency matrix or Laplacian matrix.&rdquo;</q>(-- <a href="https://en.wikipedia.org/wiki/Spectral_graph_theory" target="_blank">Wikipedia</a>)</span>
							</li>
							<li class="fragment">
								<p>So in this algorithm, we will use Similarity Graphs and their Adjacency Matrix, which we will use to create the Laplacian Matrix of the graph, or the Graph Laplacian.</p>
							</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h2>Lets start with some general definitions:</h2>
					</section>
					<section>
						<h3>Similarity Graphs</h3>
						<ul>
							<li class="fragment">
								So, we have a set of data points $ x_1, ..., x_n $ and some similarity parameter $ s_{ij} \geq 0 $ between all pairs of points $ x_i $ and $ x_j $. One way to represent the data is in the form of similarity graph $ G=(V,E) $:
								<ul>
									<li class="fragment">Each vertex $ v_i \in V$ represents a data point $ x_i $.</li>
									<li class="fragment">Two vertices $ s_i $ and $ s_j $ are connected if $ s_{ij} \geq a $, when $ a $ is 0 or other threshold, and then the edge $e=(s_i,s_j) \in E$ has weight $s_{ij}$. </li>
								</ul>
							</li>
						</ul>
						<p  class="fragment">So now we can reformulate the problem of clustering to the problem of finding a partition of the graph $G$ such that the edges between the different groups in the partition have very low (if at all) weights, and the weights of edges inside a group have a very high weights.</p>
					</section>
					<section>
						<h3>Graph Adjacency Matrix</h3>
						<ul>
							<li>
								Let $G=(V,E)$ be an undirected graph with vertex set $V=\{v_1,...,v_n\}$.
								We also assume that the graph is weighted with non-negative weights $w_{ij}$.
							</li>
							<li class="fragment">
								The weighted <i>adjacency matrix</i> of the graph is the matrix $W=(w_{ij})_{i,j=1,...,n}$.
							</li>
							<li class="fragment">
								Note:
								<ol>
									<li>
										$w_{ij}=0$ means that $v_i$ and $v_j$ are not connected by an edge.
									</li>
									<li class="fragment">
										$w_{ij}=w_{ji}$ as $G$ is undirected.
									</li>
								</ol>
							</li>
						</ul>
					</section>
					<section>
						<h3>Degree Matrix</h3>
						<ul>
							<li>
								The degree of $v_i \in V$ is defined as: \[d_i=\sum_{j=1}^n w_{ij}\]
							</li>
							<li class="fragment">
								The <i>degree matrix</i> is the diagonal matrix, when the cells on the diagonal are $d_1,...,d_n$. In other words, every value on the diagonal of $D$ is the sum of the cells of the same column in $W$.
							</li>
						</ul>
					</section>
					<section>
						<h3>Some graph partition notations</h3>
						<ul>
							<li>If $A \subset V$ is a subset of vertices, then $\bar{A}$ is the complement $V \setminus A$.</li>
							<li class="fragment">
								Vector indicator : $\mathbb{1}_A=(f_1,...,f_n)' \in \{0,1\}$, when $f_i=1$ if $v_i \in A$ and $f_i=0$ if $v_i \notin A$. We'll shortify it to $i \in A$ for $\{i \mid v_i \in A\}$.
							</li>
							<li class="fragment">
								For any two sets $A,B$(not necessarily disjoint) we can define:
								\[
								W(A,B) := \sum_{i \in A,j \in B}W_{ij}
								\]
							</li>
							<li class="fragment">
								There are two options to measure the "size" of subset $A \subset V $ :
								<ol>
									<li class="fragment">
										$|A| := \#\{v_i \in A\}$ (number of vertices in $A$).
									</li>
									<li class="fragment">
										$\mbox{vol}(A) := \sum_{i \in A}d_i$
										(Summing over the weights of all the edges attached to vertices in $A$).
									</li>
								</ol>
							</li>
						</ul>
					</section>
					<section>
						<h3>graph connectivity</h3>
						<ul>
							<li>
								$A \subset V$ is <i>connected</i> if any two vertices in $A$ can be joined by path, that does not leave $A$ (every vertex on the path is in $A$).
							</li>
							<li class="fragment">
								$A \subset V$ is called <i>connected component</i> if it is connected and there are no connections between vertices in $A$ and $\bar{A}$.
							</li>
							<li class="fragment">
								The nonempty set $ A_1,...,A_k $ form a <i>partition of $G$</i> if $A_i \cap A_j = \emptyset $ ($\forall i,j \in [1,k], i\neq j$) and $ A_1 \cup \cdot\cdot\cdot \cup A_k= V$.
							</li>
						</ul>
					</section>
					<section id="graph_types">
						<h3>Types of similarity graphs</h3>
						<p>We'll mention the popular constructions to transform our data points into graph:</p>
						<ul>
							<li class="fragment">
								<strong>The $\epsilon$-neighborhood graph:</strong> Connect all points whose pairwise similarities/distances are smaller than $\epsilon$.
							</li>
							<li class="fragment">
								<strong>$k$-nearest neighbor graphs:</strong> Connect vertex $v_i$ with vertex $v_j$ if $v_j$ is among the $k$-nearest neighbors of $v_i$. The result here is a directed graph, so there are two undirected versions of it:
								<ul>
									<li class="fragment">
										<i>$k$-nearest neighbor graphs</i> - connect $v_i$ and $v_j$ if $v_i$ is $k$-neighbor of $v_j$ <strong>or</strong> if $v_j$ is $k$-neighbor of $v_i$.
									</li>
									<li class="fragment">
										<i>Mutual $k$-nearest neighbor graphs</i> - connect $v_i$ and $v_j$ if $v_i$ is $k$-neighbor of $v_j$ <strong>and</strong> if $v_j$ is $k$-neighbor of $v_i$.
									</li>
								</ul>
							</li>
							<li class="fragment">
								<strong>The fully connected graph:</strong> Connect all points with the positive similarity as edge's weight. This type is only useful if the similarity function itself models local neighborhoods. As example we can take the Gaussian similarity function:
								\[
								s(x_i,x_j)=e^{{-\|x_i-x_j\|^2}/{2\sigma^2}}
								\]
								where $\sigma$ controls the width of the neighborhood (similar to the $\epsilon$ before).
							</li>
						</ul>
						<p class="fragment">All those graphs are used in spectral clustering. There is no info on how the choice of method influence the results.</p>
					</section>
				</section>
				<section>
					<section>
						<h2>Graph Laplacians</h2>
						<h3>And their (basic) properties</h3>
					</section>
					<section>
						<h3>Graph Laplacians</h3>
						<p>As we mentioned earlier, the concept of Spectral Clustering is coming from the field of <i>spectral graph theory</i>, in which graph Laplacians matrices are the primary tool. So they will be primary tool in spectral clustering as well.</p>
						<p class="fragment">We are now going to define the graph Laplacians and mention their most important properties.</p>
						<p class="fragment">Note: There is no unique convention which matrix exactly is called "graph Laplacians". <br />Mostly, every author gives is own version of it.</p>
						<p class="fragment"><strong>Pre-assumptions:</strong></p>
						<ul>
							<li class="fragment">
								Graph $G$ is an undirected, weighted graph with weight matrix $W$, where $w_{ij}=w_{ji}\geq0$.
							</li>
							<li class="fragment">
								Eigenvectors of a matrix not necessarily have to be normalized. (For example, the vector $e_i$ and the vector $ae_i$ for some $a\neq0$ will be considered the same vector).
							</li>
							<li class="fragment">
								Eigenvalues will be ordered increasingly, respecting multiplicities.
							</li>
							<li class="fragment">
								By "the first $k$ eigenvectors" we refer to the eigenvectors corresponding to the $k$ smallest eigenvalues.
							</li>
						</ul>
					</section>
					<section id="unnormalized_lapl">
						<h3>Unnormalized Graph Laplacians</h3>
						<p>Considering D - the <i>degree matrix</i> of $G$, and $W$ - the weight matrix of $G$, we get the <strong><i>unnormalized graph Laplacian</i></strong>:
						\[
							L=D-W
							\]
						</p>
						<p class="fragment">Note: self edges, which "sits" on the diagonal of the adjacency matrix $W$, do not change the graph Laplacians.</p>
						<p class="fragment"><strong>Lets introduce some important properties of $L$:</strong></p>
						<ol>
							<li class="fragment">
								For every vector $f \in \mathbb{R}^n$ we have:
								$f'Lf=\frac{1}{2}\sum_{i,j=1}^nw_{ij}(f_i-f_j)^2$
							</li>
							<li class="fragment">
								$L$ is symmetric and positive semi-definite.
							</li>
							<li class="fragment">
								The smallest eigenvalue of $L$ is $0$, the corresponding eigenvector is the constant one vector $\mathbb{1}$.
							</li>
							<li class="fragment">
								$L$ has $n$ non-negative, real-valued eigenvalues $0=\lambda_1\leq\lambda_2\leq\cdot\cdot\cdot\leq\lambda_n$.
							</li>
						</ol>
						<br />
						<br />
						<!--<p class="fragment">The <i>proof</i> of the 4 properties can be found in the appendix <a href="#proof1">here</a>.</p>-->
					</section>
					<section id="spectrum_of_L">
						<h3>Unnormalized Graph Laplacians</h3>
						<h4>Number of connected components and the spectrum of $L$</h4>
						<p>Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue $0$ of $L$ equals the number of connected components $A_1,...,A_k$ in the graph. The eigenspace of eigenvalue $0$ is spanned by the indicator vectors $\mathbb{1}_{A_1},...,\mathbb{1}_{A_k}$ of those components.</p>
						<br />
						<br />
						<!--<p class="fragment">The <i>proof</i> can be found in the appendix <a href="#proof2">here</a>.</p>-->
					</section>
					<section>
						<h3>The normalized graph Laplacians</h3>
						<p>There are two matrices which are called normalized graph Laplacians in the literature. Both of them are closely related to each other:</p>
						<p class="fragment">
							A symmetric normalized graph Laplacian:
						\[
							L_{sym}:=D^{-1/2}LD^{-1/2}=I-D^{-1/2}WD^{-1/2}
							\]</p>
						<p class="fragment">
							A Random Walk like normalized graph Laplacian:
						\[
							L_{rw}:=D^{-1}L=I-D^{-1}W
							\]</p>
					</section>
					<section id="prop_of_L_and_L_0">
						<h3>The normalized graph Laplacians</h3>
						<h4>Properties of $L_{sym}$ and $L_{rw}$</h4>
						<ol>
							<li class="fragment">
								For every vector $f \in \mathbb{R}^n$ we have:
								\[
								f'L_{sym}f=\frac{1}{2}\sum_{i,j=1}^nw_{ij}\Bigg(\frac{f_i}{\sqrt{d_i}}-\frac{f_j}{\sqrt{d_j}}\Bigg)^2
								\]
							</li>
							<li class="fragment">
								$\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $\lambda$ is an eigenvalue of $L_{sym}$ with eigenvector $w=D^{1/2}u$.
							</li>
							<li class="fragment">
								$\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $\lambda$ and $u$ solve the generalized eigen-problem $Lu=\lambda Du$.
							</li>
							<li class="fragment">
								$0$ is an eignevalue of $L_{rw}$ with the constant one vector $\mathbb{1}$ as eigenvector. $0$ is an eigenvalue of $L_{sym}$ with eigenvector $D^{1/2}\mathbb{1}$.
							</li>
							<li class="fragment">
								$L_{sym}$ and $L_{rw}$ are positive semi-definite and have $n$ non-negative real-valued eigenvalues $0=\lambda_1\leq\lambda_2\leq\cdot\cdot\cdot\leq\lambda_n$.
							</li>
						</ol>
						<br />
						<br />
						<!--<p class="fragment">The <i>proof</i> of the 5 properties can be found in the appendix <a href="#proof3">here</a>.</p>-->
					</section>
					<section id="spectrum_of_L_and_L">
						<h3>The normalized Graph Laplacians</h3>
						<h4>Number of connected components, the spectrum of $L_{sym}$ & $L_{rw}$</h4>
						<p>Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue $0$ of both $L_{rw}$ and $L_{sym}$ equals the number of connected components $A_1,...,A_k$ in the graph. <br />For $L_{rw}$, the eigenspace of $0$ is spanned by the indicator vectors $\mathbb{1}_{A_1},...,\mathbb{1}_{A_k}$ of those components. <br />For $L_{sym}$, the eigenspace of $0$ is spanned by the vectors $D^{1/2}\mathbb{1}_{A_i}$.</p>
						<br />
						<br />
						<!--<p class="fragment">The <i>proof</i> can be found in the appendix <a href="#proof4">here</a>.</p>-->
					</section>
				</section>
				<section>
					<section>
						<h2>Spectral Clustering Algorithms</h2>
					</section>
					<section>
						<h3>Spectral Clustering Algorithms</h3>
						<p>In this part we going to see the most common spectral clustering algorithms.</p>
						<p class="fragment">Some assumptions before we start:</p>
						<ol>
							<li class="fragment">Our data consists of $n$ "points" $x_1,...,x_n$ which can be arbitrary objects.</li>
							<li class="fragment">The pairwise similarities $s_{ij}=s(x_i,x_j)$ is defined by some similarity function that is symmetric and non-negative.</li>
							<li class="fragment">The similarity matrix is defined as $S=(s_{ij})_{i,j=1,...,n}$</li>
							<li class="fragment">As mentioned before, there is no info about which way of constructing similarity graph is better. So the method of construction is (for now) not an issue.</li>
						</ol>
					</section>
					<section>
						<h3>Unnormalized spectral clustering algorithm</h3>
						<p><strong>Input:</strong> Similarity matrix $S \in \mathbb{R}^{n\times n}$, $k$ of clusters to construct.</p>
						<ul style="font-size: 30px;">
							<li class="fragment">Construct a similarity graph, get $W$ - its weighted adjacency matrix.</li>
							<li class="fragment">Compute the unnormalized Laplacian $L$.</li>
							<li class="fragment"><strong>Compute the first $k$ eigenvectors $u_1,...,u_k$ of $L$.</strong></li>
							<li class="fragment">Let $U \in \mathbb{R}^{n \times k}$ be the matrix with the vectors $u_1,...,u_k$ as columns.</li>
							<li class="fragment">For $i=1,...,n$, let $y_i \in \mathbb{R}^k$ be the $i$-th vector row of $U$.</li>
							<li class="fragment">Cluster the points $(y_i)_{i=1,...,n} \in \mathbb{R}^k$ with the $k$-means algorithm into clusters $C_1,...,C_k$.</li>
						</ul>
						<p class="fragment"><strong>Output:</strong> Clusters $A_1,...,A_k$ with $A_i=\{j \mid y_j \in C_j\}$</p>
					</section>
					<section>
						<h3>Normalized spectral clustering algorithm</h3>
						<h6>By $L_{rw}$, according to <i>Shi and Malik (2000)</i></h6>
						<p><strong>Input:</strong> Similarity matrix $S \in \mathbb{R}^{n\times n}$, $k$ of clusters to construct.</p>
						<ul style="font-size: 30px;">
							<li class="fragment">Construct a similarity graph, get $W$ - its weighted adjacency matrix.</li>
							<li class="fragment">Compute the unnormalized Laplacian $L$.</li>
							<li class="fragment"><strong>Compute the first $k$ generalized eigenvectors $u_1,...,u_k$ of the generalized eigenproblem $Lu=\lambda Du$.</strong></li>
							<li class="fragment">Let $U \in \mathbb{R}^{n \times k}$ be the matrix with the vectors $u_1,...,u_k$ as columns.</li>
							<li class="fragment">For $i=1,...,n$, let $y_i \in \mathbb{R}^k$ be the $i$-th vector row of $U$.</li>
							<li class="fragment">Cluster the points $(y_i)_{i=1,...,n} \in \mathbb{R}^k$ with the $k$-means algorithm into clusters $C_1,...,C_k$.</li>
						</ul>
						<p class="fragment"><strong>Output:</strong> Clusters $A_1,...,A_k$ with $A_i=\{j \mid y_j \in C_j\}$</p>
					</section>
					<section>
						<h3>Normalized spectral clustering algorithm</h3>
						<h6>By $L_{sym}$, according to <i>Ng, Jordan and Weiss (2002)</i></h6>
						<p><strong>Input:</strong> Similarity matrix $S \in \mathbb{R}^{n\times n}$, $k$ of clusters to construct.</p>
						<ul style="font-size: 30px;">
							<li class="fragment">Construct a similarity graph, get $W$ - its weighted adjacency matrix.</li>
							<li class="fragment">Compute the normalized Laplacian $L_{sym}$.</li>
							<li class="fragment"><strong>Compute the first $k$ eigenvectors $u_1,...,u_k$ of $L_{sym}$.</strong></li>
							<li class="fragment">Let $U \in \mathbb{R}^{n \times k}$ be the matrix with the vectors $u_1,...,u_k$ as columns.</li>
							<li class="fragment"><strong>Form the matrix $T \in \mathbb{R}^{n \times k}$ from $U$ by normalizing the rows to norm 1: $t_{ij}=u_{ij}/(\sum_ku_{ik}^2)^{1/2}$.</strong></li>
							<li class="fragment">For $i=1,...,n$, let $y_i \in \mathbb{R}^k$ be the $i$-th vector row of $T$.</li>
							<li class="fragment">Cluster the points $(y_i)_{i=1,...,n} \in \mathbb{R}^k$ with the $k$-means algorithm into clusters $C_1,...,C_k$.</li>
						</ul>
						<p class="fragment"><strong>Output:</strong> Clusters $A_1,...,A_k$ with $A_i=\{j \mid y_j \in C_j\}$</p>
					</section>
					<section>
						<h3>spectral Clustering Algorithm</h3>
						<h4>Some notions:</h4>
						<ul>
							<li class="fragment">All three algorithms looks rather similar, apart from the fact that they use three different graph Laplacians.</li>
							<li class="fragment">The main trick in all three is to change the representation of the abstract data points $x_i$ to points $y_i \in \mathbb{R}^k$.</li>
							<li class="fragment">Due to the properties of the graph Laplacians, this change of representation of the points enhances the cluster-properties in the data - making the detection of clusters trivial task.</li>
							<li class="fragment">In our algorithms, we did it with the simple $k$-means clustering algorithm - with no difficulties.</li>
						</ul>
					</section>
				</section>

				<section>
					<section>
						<h2>Graph cut point of view</h2>
					</section>
					<section>
						<h3>Graph cut point of view</h3>
						<p>As we know, clustering is to seperate points in different groups according to their similarities.</p>
						<p class="fragment">If the data is given in form of a similarity graph, we can restate the clustering problem as <i>graph partitioning</i> problem, such that the edges between different groups of the graph have a very low weight, and the edges within a group have high weight.</p>
						<p class="fragment">The simplest and most direct way to construct a partition of a graph is to solve the <strong>mincut</strong> problem: For a given number $k$ of subsets - choose a partition $A_1,...,A_k$ that minimizes
							\[
							\mbox{cut}(A_1,...,A_k):=\frac{1}{2}\sum_{i=1}^kW(A_i,\bar{A_i})
							\]
							<br />Here, we use the notation $W(A,B):=\sum_{i \in A,j \in B}w_{ij}$ that we seen before, and $\bar{A}$ for the complement of $A$.
						</p>
					</section>
					<section>
						<h3>Mincut - cont.</h3>
						<p>For $k=2$, mincut is relatively easy problem to solve, even efficiently. But, in practice, sometimes the result is not what we want: In many cases, mincut simply separates one individual vertex from the rest.</p>
						<p class="fragment">One way to solve it, is to explicitly request that the sets $A_1,...,A_k$ are "reasonably large". The two most common functions to encode this are RatioCut and the normalized cut Ncut.</p>
						<p class="fragment">
							\[
							\mbox{RatioCut}(A_1,...,A_k):=\frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\bar{A_i})}{|A_i|}=\sum_{i=1}^k\frac{\mbox{cut}(A_i,\bar{A_i})}{|A_i|} \\
							\mbox{Ncut}(A_1,...,A_k):=\frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\bar{A_i})}{\mbox{vol}(A)}=\sum_{i=1}^k\frac{\mbox{cut}(A_i,\bar{A_i})}{\mbox{vol}(A)}
							\]
						</p>
						<p class="fragment">The difference between RatioCut and Ncut is how they defined the size of a subset $A_i$: <br />RatioCut define it as the the number of vertices in $A_i$: $|A_i|$.<br />Ncut define it as the weights of the edges in $A_i$: $\mbox{vol}(A)$.</p>
					</section>
					<section>
						<h3>RatioCut & Ncut - cont.</h3>
						<p>As easily seen in the formulas of RatioCut and Ncut, both of them are minimized if the respective sizes of the clusters $A_i$ are large "together", or in other words - balanced.</p>
						<p class="fragment">This added condition (balanced clusters) makes the previously simple problem to solve now a NP hard.</p>
						<p class="fragment">Spectral clustering is a way to solve relaxed versions of those problems.</p>
					</section>
					<section>
						<h3>Approximating RatioCut for $k=2$</h3>
						<p>Our goal is to solve the optimization problem
							\[
							\min_{A\subset V}{\mbox{RatioCut}(A,\bar{A})} \tag{1}
							\]</p>
						<p class="fragment">We're going to try to write it in a better way. <br />For given subset $A \subset V$ we defined $f=(f_1,...,f_n)'\in \mathbb{R}^n$ such that:
							<br /><br />\[
							f_i=
							\left\{
							\begin{array}{ll}
							\sqrt{|\bar{A}|/|A|}  & \mbox{if } v_i \in A \\
							-\sqrt{|A|/|\bar{A}|}  & \mbox{if } v_i \in \bar{A}
							\end{array}
							\right.
							\tag{2}
							\]</p>
						<p class="fragment">Now we can rewrite the RatioCut function with the unnormalized graph Laplacian:
							<span style="font-size: 22px;">\[
							f'Lf=\frac{1}{2}\sum_{i,j=1}^nw_{ij}(f_i-f_j)^2
							=\frac{1}{2}\sum_{i\in A,j\in \bar{A}}w_{ij}\Bigg(\sqrt{\frac{\bar{A}}{A}}+\sqrt{\frac{A}{\bar{A}}}\Bigg)^2 + \\
							\frac{1}{2}\sum_{i\in \bar{A},j\in A}w_{ij}\Bigg(-\sqrt{\frac{\bar{A}}{A}}-\sqrt{\frac{A}{\bar{A}}}\Bigg)^2
							=\mbox{cut}(A,\bar{A})\Bigg(\frac{|\bar{A}|}{|A|}+\frac{|A|}{|\bar{A}|}+2\Bigg)\\
							=\cdot\cdot\cdot=|V|\cdot\mbox{RatioCut}(A,\bar{A})
							\]</span></p>
					</section>
					<section>
						<h3>Apprx. RatioCut for $k=2$ - cont.</h3>
						<p>Additionally, we have
							<br /><p>\[
						\sum_{i=1}^nf_i=\sum_{i \in A}{\sqrt{\frac{|\bar{A}|}{|A|}}}-\sum_{i \in \bar{A}}{\sqrt{\frac{|A|}{|\bar{A}|}}}=|A|{\sqrt{\frac{|\bar{A}|}{|A|}}}-|\bar{A}|{\sqrt{\frac{|A|}{|\bar{A}|}}}=0
							\]</p></p>
						<p class="fragment">Means, the vector $f$ from Eq. $(2)$ is orthogonal to the vector $\mathbb{1}$. Finally, $f$ satisfies:
							<br />
							\[
							\|f\|^2=\sum_{i=1}^nf_i^2=|A|\frac{|\bar{A}|}{|A|}+|\bar{A}|\frac{|A|}{|\bar{A}|}=|\bar{A}|+|A|=n
						\]</p>
						<p class="fragment">Altogether the problem of minimizing $(1)$ can be rewritten as :
							<br />\[
							\mbox{min}_{A \subset V}f'Lf \;\;\mbox{s.t.}\;\; f\perp \mathbb{1},f_i \;\;\mbox{as defined in Eq.}\;(2), \|f\|=\sqrt{n} \tag{3}
						\]</p>
						<p class="fragment">This is a discrete optimization problem ($f$ entries only allowed to take two particular values) and of course it is still NP hard. The most obvious relaxation in this setting is to allow that $f_i$ takes any value in $\mathbb{R}$.</p>
						<p class="fragment">So we get the following relaxed optimization problem:
						<br />
							\[
							\mbox{min}_{f \in \mathbb{R}^n}f'Lf \;\;\mbox{s.t.}\;\; f\perp \mathbb{1}, \|f\|=\sqrt{n} \tag{4}
							\]
						</p>
					</section>
					<section>
						<h3>Apprx. RatioCut for $k=2$ - cont.</h3>
						<p>By Rayleigh-Ritz theorm, the solution of this problem is given by the 2nd smallest eigenvector of $L$ (the 1st smallest is value $0$ with vector $\mathbb{1}$).</p>
						<p class="fragment">So we can approximate a minimizer of RatioCut by the 2nd eigenvector of $L$. But, we need to transform the real-valued solution into a discrete indicator vector. The simplest way to do that is:
						<br />
							\[
							\left\{
							\begin{array}{ll}
							v_i \in A  & \mbox{if } f_i \geq 0 \\
							v_i \in \bar{A}  & \mbox{if } f_i < 0
							\end{array}
							\right.
						\]
						</p>
						<p class="fragment">But, for larger $k$ values, that way is too simple. What most spectral clustering algorithms do, is treat the coordinates $f_i$ as points in $\mathbb{R}$ and cluster them into two groups $C,\bar{C}$ by the $k$-means method.
							<br />
							\[
							\left\{
							\begin{array}{ll}
							v_i \in A  & \mbox{if } f_i \in C \\
							v_i \in \bar{A}  & \mbox{if } f_i \in \bar{C}
							\end{array}
							\right.
							\]</p>
						<p class="fragment">This is exactly the <i>unnormalized spectral clustering</i> algorithm for the case of $k=2$.</p>
					</section>
					<section>
						<h3>Approximating RatioCut for any $k$</h3>
						<p>Given a partition of $V$ into $k$ sets $A_1,...,A_k$, we defined $k$ indicator vectors $h_j=(h_{1,j},...,h_{n,j})'$ by
						<br />
						\[
							h_{i,j}=
							\left\{
							\begin{array}{ll}
							1/\sqrt{|A_j|} & \mbox{if } v_i \in A_j \\
							0 & \mbox{otherwise}
							\end{array}
							\right.
							\quad
							\quad
							(i=1,...,n;j=1,...,k)
							\tag{5}
						\]</p>
						<p class="fragment">Then we set the matrix $H \in \mathbb{R}^{n \times k}$ such that its columns are $h_{i,j}$. Also we can see that H columns' are orthonormal o each other - $H'H=I$. So we get:
						<br />
						\[
							h_i'Lh_i=\frac{\mbox{cut}(A_i,\bar{A_i})}{|A_i|}=(H'LH)_{ii}
						\]</p>
						<p class="fragment">And using all that we get: <br />
						\[
							\mbox{RatioCut}(A_1,...,A_k)=\sum_{i=1}^kh_i'Lh_i=\sum_{i=1}^k(H'LH)_{ii}=\mbox{Tr}(H'LH)
						\]</p>
					</section>
					<section>
						<h3>Approximating RatioCut for any $k$ - cont.</h3>
						<p>So now we can rewrite the problem of minimizing $\mbox{RatioCut}(A_1,...,A_k)$ as: <br /><br />
						\[
							\mbox{min}_{A_1,...,A_k} \mbox{ Tr}(H'LH) \;\;\mbox{s.t.}\;\; H'H=I, H \;\;\mbox{as defined in Eq.}\; (5)
						\]</p>
						<p class="fragment">Same as the $k=2$ case, we can relax the problem by allowing it to get real-values entries, and: <br />
						\[
							\mbox{min}_{H \in \mathbb{R}^{n \times k}} \mbox{ Tr}(H'LH) \;\;\mbox{s.t.}\;\; H'H=I
						\]</p>
						<p class="fragment">Again, we need to re-convert the real valued solution matrix into a discrete partition.</p>
						<br />
						<p class="fragment">We can see that the matrix $H$ is in fact the matrix $U$ used in the unnormalized spectral clustering algorithm. As in the $k=2$ case, we now use the $k$-means method on the rows of $U=H$.</p>
					</section>
					<section>
						<h3>Approximating Ncut</h3>
						<p>The process for Ncut is quiet similar to the one we did for RatioCut.</p>
						<p class="fragment">In the case $k=2$, we define the cluster indicator vector $f$ by:
						<br />
						\[
							f_i=
							\left\{
							\begin{array}{ll}
							\sqrt{\frac{\mbox{vol}(\bar{A})}{\mbox{vol}(A)}} & \mbox{if } v_i \in A \\
							-\sqrt{\frac{\mbox{vol}(A)}{\mbox{vol}(\bar{A})}}  & \mbox{if } v_i \in \bar{A}
							\end{array}
							\right.
							\tag{6}
						\]</p>
						<p class="fragment">As before, $(Df)'\mathbb{1}=0,f'Df=\mbox{vol}(V), \mbox{ and } f'Lf=\mbox{vol}(V)\mbox{Ncut}(A,\bar{A})$.</p>
						<p class="fragment">So minimizing Ncut problem can be rewritten as:
						<br />
						\[
							\mbox{min}_Af'Lf \;\;\mbox{s.t.}\;\; f \;\;\mbox{as in}\;\; (6), Df \perp \mathbb{1}, f'Df=\mbox{vol}(V). \tag{7}
						\]</p>
						<p class="fragment">Again, relax the problem to allow real values:
						<br />
						\[
						\mbox{min}_{f \in \mathbb{R}^n}f'Lf \;\;\mbox{s.t.}\;\; Df \perp \mathbb{1}, f'Df=\mbox{vol}(V). \tag{8}
						\]</p>
					</section>
					<section>
						<h3>Approximating Ncut - cont. ($k=2$)</h3>
						<p>Now we substitute $g:=D^{1/2}f$:<br />
							\[
							\mbox{min}_{g \in \mathbb{R}^n}g'\underbrace{D^{-1/2}LD^{-1/2}}_{=L_{sym}}g\;\;\mbox{s.t.}\;g\perp D^{1/2}\mathbb{1},\;\|g\|^2=\underbrace{\mbox{vol}(V)}_{=\mbox{const}}. \tag{9}
							\]</p>
						<p class="fragment">Also notice that $D^{1/2}\mathbb{1}$ is the first eigenvector of $L_{sym}$. Then, problem $(9)$ is in the form of Rayleigh-Ritz theorem, thus the solution is given by the 2nd eigenvector of $L_{sym}$.</p>
						<p class="fragment">Re-substituting $f=D^{-1/2}g$ and using <a href="#prop_of_L_and_L_0">the properties of $L_{sym}$</a> we see that $f$ is the 2nd eigenvector of $L_{rw}$, or the equivalently generalized eigenvector of $Lu=\lambda Du$.</p>
					</section>
					<section>
						<h3>Approximating Ncut for any $k$</h3>
						<p>For $k>2$, we defined the indicator vectors $h_j=(h_{1,j},...,h_{n,j})'$ by
							<br />
							\[
							h_{i,j}=
							\left\{
							\begin{array}{ll}
							1/\sqrt{\mbox{vol}(A_j)} & \mbox{if } v_i \in A_j \\
							0 & \mbox{otherwise}
							\end{array}
							\right.
							\quad
							\quad
							(i=1,...,n;j=1,...,k)
							\tag{10}
							\]</p>
						<p class="fragment">Then we set the matrix $H \in \mathbb{R}^{n \times k}$ such that its columns are $h_{i,j}$. Also - $H'H=I, h'_iDh_i=\mathbb{1}, h'_iLh_i=\mbox{cut}(A_i,\bar{A_i})/\mbox{vol}(A_i)$. So we get:
							<br />
							\[
							\mbox{min}_{A_1,...,A_k} \mbox{ Tr}(H'LH) \;\;\mbox{s.t.}\;\; H'DH=I, H \;\;\mbox{as in Eq.}\; (10)
							\]</p>
						<p class="fragment">Realxing the discreteness condition and substituting $T=D^{1/2}H$   we get:<br />
							\[
							\mbox{min}_{T \in \mathbb{R}^{n \times k}} \mbox{ Tr}(T'D^{-1/2}LD^{-1/2}T) \;\;\mbox{s.t.}\;\; T'T=I
							\]</p>
						<p class="fragment">Re-substituting $H=D^{-1/2}T$ we see that the solution $H$ consists of the first $k$ eigenvectors of $L_{rw}$, or of $Lu=\lambda Du$.</p>
						<p class="fragment">This yields the normalized spectral clustering algorithm by $L_{rw}$.</p>
					</section>
					<section>
						<h3>Comments on relaxation approach</h3>
						<p>One important fact to remember: There is no guarantee on the quality of the solution of the relaxed problem, in compare with the exact solution. The diff between them can be large. Lets look on the next graph:
						<img data-src="imgs/figure2.png" style="float:right; padding: 0 0 10px 10px;" class="fragment" /></p>
						<p class="fragment">This graph is called <i>The cockroach graph</i> (Guattery and Miller (1998)). The ideal cut for $k=2$ will obviously be cutting the edges $(v_k,v_{k+1}),(v_{3k},v_{3k+1})$, which will give RatioCut value of $2/k$. But it can be showed that unnormalized spectral clustering always cuts the vertical edges, causing the RatioCut value to be $1$.</p>
						<p class="fragment">In general it is known that there are no efficient algorithms to approximate balanced graph cuts - and it is even NP hard problem.</p>
						<p class="fragment">But, still the spectral relaxation is appealing and popular due to the fact that it results in a standard linear algebra problem, which is simple to solve.</p>
					</section>
				</section>
				<section>
					<section>
						<h2>Random walks point of view</h2>
					</section>
					<section>
						<h3>Random walks</h3>
						<p>Another way to explain spectral clustering is based on random walks on the similarity graph. A <i>random walk</i> on a graph is a stochastic process, which randomly jumps from vertex to vertex.</p>
						<p class="fragment">The goal of spectral clustering here is to find a partition such that the random walk stays long within the same cluster, and seldom jumps between clusters.</p>
						<p class="fragment">Adding that to mincut problem - a graph with low cut will "force" the random walk to stay on the same cluster, because it will have less "exits".</p>
					</section>
					<section>
						<h3>Random walks - cont.</h3>
						<p>The transition probability of jumping in one step from vertex $v_i$ to $v_j$ is proportional to $w_{ij}$, by $p_{ij}:=w_{ij}/d_i$. And we can define the transition matrix $P=(p_{ij})_{i,j=1,...,n}$ will be:
							<br />
							\[
							P=D^{-1}W
							\]</p>
						<p class="fragment">If the graph is connected and not-bipartite, the random walk always possesses a unique stationary distribution $\pi=(\pi_1,...,\pi_n)'$, where $\pi_i=d_i/\mbox{vol}(V)$.</p>
						<p class="fragment">Also, $L_{rw}=I-D^{-1}W = I-P$, which means that $\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $1-\lambda$ is an eigenvalue of $P$ with eigenvector $u$.</p>
						<p class="fragment">So we get that the largest eigenvectors of $P$ and the smallest eigenvectors of $L_{rw}$ can be used to describe cluster properties of the graph.</p>
					</section>
					<section id="rw_first_prop">
						<h3>Random walks - cont.</h3>
						<h4>Ncut via transition probabilities</h4>
						<p>Let $G$ be connected and non-bipartite. Assume that we run the random walk $(X_t)_{t\in\mathbb{N}}$ starting with $X_0$ in the stationary distribution $\pi$. For disjoint subsets $A,B \subset V$, denote by $P(B\mid A):=P(X_1 \in B \mid X_0 \in A)$. Then:
						<br />
						\[
							\mbox{Ncut}(A,\bar{A})=P(\bar{A}\mid A) + P(A \mid \bar{A}).
						\]</p>
						<!--<p class="fragment">The <i>proof</i> can be found in the appendix <a href="#proof5">here</a>.</p>-->
						<p class="fragment">This leads to a nice interpretation of Ncut and normalized spectral clustering: when we minimize Ncut, we actually searching for a cut such that a random walk seldom transitions $A$ to $\bar{A}$ and vice versa.</p>
					</section>
					<section>
						<h3>Random walks - cont.</h3>
						<h4>The commute distance</h4>
						<p>The <i>commute distance</i> (also called <i>resistance distance</i>) $c_{ij}$ is the expected time it takes the random walk to travel from $v_i$ to $v_j$ and back.</p>
						<p class="fragment">The commute distance has several nice properties, which making it particularly appealing for machine learning. As opposed to the <i>shortest path distance</i> on a graph, the commute distance decreases if there are many different short ways from $v_i$ to $v_j$.</p>
						<p class="fragment">So commute distance looks at the set of short paths. By commute distance, two points are closer not only by the short path between them, but also if they lies in the same high-density region of the graph. So commute distance is well-suited for clustering purposes.</p>
					</section>
					<section>
						<h3>Random walks - cont.</h3>
						<h4>The commute distance - cont.</h4>
						<p>The commute distance on a graph can be computed with the help of the generalized inverse (pseudo-inverse, Moore-Penrose inverse) $L^\dagger$ of the graph Laplacian $L$.</p>
						<p class="fragment">We've seen that $L$ can be decomposed as $L=U\Lambda U'$, where $U$ columns are the eigenvectors, and $\Lambda$ is the diagonal matrix with the eigenvalues $\lambda_1,...,\lambda_n$ on the diagonal. As at least one eigenvalue is $0$, $L$ is not invertible. So, we define its generalized inverse as: $L^\dagger:=U\Lambda^\dagger U'$ where the matrix $\Lambda^\dagger$ is the diagonal matrix with diagonal entries $1/\lambda_i$ if $\lambda_i \neq 0$ and $0$ if $\lambda_i=0$. <br />$L^\dagger$ is positive semi-definite and symmetric, and the entries of $L^\dagger$ can be computed as:  $l_{ij}^\dagger = \sum_{k=2}^n \frac{1}{\lambda_k}u_{ik}u_{jk}$. </p>
						<p class="fragment">So, lets say we have an undirected, connected graph, and we want to compute the commute distance. That can be done by:
						<br />
						\[
							c_{ij}=\mbox{vol}(V)(l_{ii}^\dagger-2l_{ij}^\dagger+l_{jj}^\dagger)=\mbox{vol}(V)(e_i-e_j)'L^\dagger(e_i-e_j)
						\]</p>
					</section>
					<section>
						<h3>Random walks - cont.</h3>
						<h4>The commute distance - cont.</h4>
						<p>There are other ways to express $c_{ij}$, with use of $L_{sym}$ or determinants of certain sub-matrices of $L$, but the reason of choosing this expression is this: It shows that $\sqrt{c_{ij}}$ can be considered as an Euclidean distance function on the vertices of the graph.</p>
						<p class="fragment">This means that we can construct an embedding which maps the vertices $v_i$ of the graph on points $z_i \in \mathbb{R}^n$ such that the Euclidean distance between the points $z_i$ coincide with the commute distance on the graph: Choose $z_i \in \mathbb{R}^n$ corresponding to the $i$-th row of $U(\Lambda^\dagger)^{1/2}$, and we get:
						<br />
						\[
							\langle z_i,z_j \rangle = e_i'L^\dagger e_j \\
							c_{ij}=\mbox{vol}(V)\|z_i-z_j\|^2

						\]</p>
						<p class="fragment">But still the embedding used in spectral clustering and commute distance are not identical. There are some differences, and without making further assumptions (like choosing a strictly positive similarity function), the relation between them is rather loose.</p>
					</section>
				</section>
				<section>
					<section>
						<h2>Practical Details</h2>
						<h4>(Or, how to start to implement spectral clustering)</h4>
					</section>
					<section>
						<h3>Constructing the similarity graph</h3>
						<p>Constructing the similarity graph for spectral clustering is not a trivial task, and little is known on theoretical implications of the different constructions.</p>
						<p class="fragment">First, we need to decide which <strong>similarity function</strong> to use. Obviously, we need to choose a function that will have some meaning for our data points - that the function will give high similarity to points from same group, and the opposite.</p>
						<p class="fragment">More over, the global "long-range" behavior of the similarity function is not so important - two points won't be connected whether they have similarity of $0.01$ or $0.001$.</p>
						<p class="fragment">Usually, when the data points live in the Euclidean space $\mathbb{R}^d$, we'll take the Gaussian similarity function:
							\[
							s(x_i,x_j)=e^{{-\|x_i-x_j\|^2}/{2\sigma^2}}
							\]</p>
						<p class="fragment">If we conclude this, the choice of the similarity function depends on the domain the data comes from. There are no general advices that can be given.</p>
					</section>
					<section>
						<h3>Which type of similarity graph</h3>
						<p>We mentioned <a href="#graph_types">earlier</a> 3 different types of similarity graphs.</p>
						<p class="fragment">Later on we will view a toy example, which then we will discuss the different types of graphs and their properties.</p>
						<p class="fragment">As a spoiler of the conclusion their, it's generally recommended to work with the $k$-nearest neighbor graph as a first choice:</p>
						<ul>
							<li class="fragment">It's simple to work with.</li>
							<li class="fragment">As a result we get a sparse adjacency matrix $W$.</li>
							<li class="fragment">Among the grpahs it is the least vulnerable to unsuitable choices of parameters.</li>
						</ul>
					</section>
					<section>
						<h3>Parameters of similarity graph</h3>
						<p>How to choose the $k$ or $\epsilon$ parameters wisely? Not an easy task, specially when barely any theoretical results are known to guide us.</p>
						<p class="fragment">All the results of how connectivity of random graphs can be achieved holds only in the case when $\mbox{lim}|V| \to \infty$. All those results don't really help us for choosing in case of finite $|V|$.</p>
						<p class="fragment">In general, if the similarity graph contains more connected components than the number of clusters we want to detect, then the algorithm will return connected components as clusters. which is not always the wanted result.</p>
						<p class="fragment">So, when building the similarity graph, we need to make sure that the graph is connected, or that the number of connected components or isolated vertices is much smaller than the number of clusters we want to get.</p>
					</section>
					<section>
						<h3>Parameters of similarity graph - cont.</h3>
						<p>If the graph is small or medium-sized, this can be tried out "by foot". For larger graphs, it depends on the graph:</p>
						<ul>
							<li class="fragment">$k$-nearest neighbor graph - the first approximation could be to choose $k$ in the order of $\mbox{log}(n)$.</li>
							<li class="fragment">Mutual $k$-nearest neighbor graph - tricky task. This graph obviously has less edges than it's not-mutual brother - which causes the graph to have more connected components or more isolated vertices. Not so good. So our general goal will be to choose $k$ really high. But there are no records on how to do it efficiently.</li>
							<li class="fragment">The $\epsilon$-neighborhood graph - We need to choose $\epsilon$ as the length of the longest edge in a minimal spanning tree of the fully connected graph (Which can be determined by any minimal spanning tree algorithm). However, in case the data contains outliers, or in case the data contains several tight clusters which are very far apart from each other, our $\epsilon$ will be too large to work with.</li>
							<li class="fragment">Fully connected graph - if used with a similarity function (like the Gaussian), then the scale of the function should be chosen such that it returns a graph with similar properties as a corresponding $k$ or $\epsilon$ graph would have.</li>
						</ul>
					</section>
					<section>
						<h3>Parameters of similarity graph - cont.</h3>
						<p>Also, we need to make sure that for most of the data points, the similarity values the function return are not "too small" or "too high".</p>
						<p class="fragment">Specifically for the Gaussian: We can choose $\sigma$ in the order of the mean distance of a point to its $k$-th nearest neighbor (when $k \sim \mbox{log}(n)+1$, as described before). Other way, set $\epsilon$ as before, and then choose $\sigma = \epsilon$.</p>
						<p class="fragment">To summerize the choice of parameters - all the given rules are nice, but nothing is certain. It stays quite a sensitive task in spectral clustering to choose the parameters, and what can work for one set of data points, can fail ultimately for other.</p>
						<p class="fragment">Finding rules which have a theoretical justification should be considered an interesting and important topic for future researches.</p>
					</section>
					<section>
						<h3>Computing the eigenvectors step</h3>
						<p>Computing the first $k$ eigenvectors of a potentially large graph Laplacian matrix is a hard task. Luckily, using the $k$-nearest or the $\epsilon$-neighborhood graphs, we get sparse matrices, which are easier to solve using methods like the <i>power method</i> or the <i>Lanczos</i> method. The speed of convergence of those algorithms depends on the size of the eigengap (also called spectral gap): $\gamma_k=|\lambda_k-\lambda_{k+1}|$. Larger eigengap value means faster convergence.</p>
						<p class="fragment">When one of the $k$ first eigenvectors has multiplicity larger than $1$, like in case when the eigenvalue $0$ has multiplicity $k$, the eigenspace is spanned by the $k$ cluster indicator vectors, and our computed vectors won't necessarily converge to them.</p>
						<p class="fragment">But, all vectors in the space spanned by the cluster indicator vectors $\mathbb{1}_{A_i}$ have the form $u=\sum_{i=1}^ka_i\mathbb{1}_{A_i}$ for some coefficients $a_i$, so they are piecewise constant on the clusters.</p>
						<p class="fragment">So the vectors returned by the eigensolvers still contain the wanted data about the clusters, which is needed for the $k$-means algorithm to reconstruct the clusters.</p>
					</section>
					<section>
						<h3>The number of clusters</h3>
						<p>Choosing the number $k$ of clusters is a general problem for all clustering algorithms, and as such there are lots of methods to solve it. All of those methods are suitable for spectral clustering.</p>
						<p class="fragment">There is one tool which is particularly designed for spectral clustering: <i>eigengap heuristic</i>. It can be used for all three graph Laplacians. The goal is to choose the number $k$ such that all eigenvalues $\lambda_1,...,\lambda_k$ are very small, but $\lambda_{k+1}$ is relatively large.</p>
						<p class="fragment">There are several justifications for this procedure:</p>
						<ul>
							<li class="fragment">Based on perturbation theory, we observe that in the ideal case of $k$ completely disconnected clusters, the eigenvalue $0$ has multiplicity $k$, and then there is a gap to the $(k+1)$th eigenvalue $\lambda_{k+1}>0$.</li>
							<li class="fragment">Based on spectral graph theory, many geometric invariants of the graph can be expressed or bounded with the help of the first eigenvalues of the graph Laplacian. In particular, the sizes of cuts are closely related to the size of the first eigenvalues.</li>
						</ul>
					</section>
					<section>
						<h3>The number of clusters - cont.</h3>
						<p>In the toy example we will see later, we will conclude that the <i>eigengap heuristic</i> usually works well if the data contains very well pronounced clusters, but in ambiguous cases it also returns ambiguous results.</p>
						<p class="fragment">Another conclusion is, that the choice of the number of clusters and the choice of the connectivity parameters of the similarity graphs affect each other. In one hand, if the connectivity parameter breaks the graph into, lets say, $k_0$ connected components, then choosing $k_0$ as the number of clusters will be a valid choice. In the other hand, when having a connected graph, is not clear how those two values interacts.</p>
					</section>
					<section>
						<h3>The $k$-means step</h3>
						<p>The choice of $k$-means algorithm as the clustering algorithm is an arbitrary choice. For example, in the ideal case of completely separated clusters, we know that the eigenvectors of $L$ and $L_{rw}$ are peicewise constant. In this case, all points $x_i$ which belong to the same cluster $C_s$ are mapped to exactly the sample point $y_i$, namely to the unit vector $e_s\in\mathbb{R}^k$. So any clustering technique applied to the points $y_i\in\mathbb{R}^k$ will be able to extract the correct clusters.</p><!--
						<p class="fragment">No matter which clustering method we choose for this step, it is also important to look at the Euclidean distance between the points $y_i$. We seen that this distance is related to the commute distance on the graph, and in other researches also relating it to "diffusion distance".</p>
						<p class="fragment"></p>-->
					</section>
					<section>
						<h3>Which graph Laplacian to use?</h3>
						<p>When coming to choose the Laplacian we want to use, first we need to look over the degree distribution of the graph vertices. If they all have (more or less) the same degree, then all three Laplacians will do equally well.</p>
						<p class="fragment">Remember, the partition we find in clustering needs to:</p>
						<ol>
							<li class="fragment">Minimize the between-cluster similarity (minimizing $\mbox{cut}(A,\bar{A})$).</li>
							<li class="fragment">Maximize the within-cluster similarity (maximizing $W(A,A)$ and $W(\bar{A},\bar{A})$).</li>
						</ol>
						<p class="fragment">Both Ncut and RatioCut implement the first objective. But what about the second objective? We can write the within-cluster similarities in another way:
						<br />
						\[
							W(A,A)=W(A,V)-W(A,\bar{A})=\mbox{vol}(A)-\mbox{cut}(A,\bar{A})
						\]</p>
						<p class="fragment">Which means that it is maximized if $\mbox{cut}(A,\bar{A})$ is small and $\mbox{vol}(A)$ is large - and we achieve exactly that by minimizing Ncut. So Ncut implement the 2nd objective.</p>
						<p class="fragment">But RatioCut maximizes $|A|$ and $|\bar{A}|$ and not the $\mbox{vol}(A)$, and that means that RatioCut doesn't implement the 2nd objective.</p>
						<p class="fragment">So, Normalized spectral clustering (Ncut) implement both objective, while unnormalized spectral clustering (RatioCut) only implement the first.</p>
					</section>
					<section>
						<h3>Which graph Laplacian to use? - cont.</h3>
						<p>Another issue to consider is consistency: What happen when more and more points are added? Do the clustering results of spectral clustering converge to a useful partition?</p>
						<p class="fragment">For both normalized spectral clustering algorithms, it can be proved that the results do converge, as we take $n \to \infty$. In that limit, $L_{sym}$ converges to an operator $U$, which means that the eigenvectors and eigenvalues of $L_{sym}$ converges into those of $U$. Similar proof can be shown about random walk. So the consistency of both $L_{rw}$ and $L_{sym}$ hold.</p>
						<p class="fragment">That can't be said about unnormalized spectral clustering algorithm. It can be proved that $L$ doesn't converge, or converge to trivial solution, such as single point being a cluster. That's true not only for very large sample size, but even in case of small sample size. </p>
					</section>
					<section id="minimal_degree">
						<h3>Which graph Laplacian to use? - cont.</h3>
						<p>The way to prevent those problems is this: We need to make sure that the eigenvalues of $L$ used in spectral clustering are significantly smaller than the minimal degree in the graph.</p>
						<p class="fragment">The mathematical reason for this condition is that eigenvectors larger than the minimal degree approximate Dirac functions, which have value $\sim0$ in all but one coordinate. So, using those vectors for clustering will seperate the one vertex where the eigenvector is non-zero from the others.</p>
						<p class="fragment">Again, those problems occurs only with unnormalized spectral clustering. Normal spectral clustering doesn't suffer from those issues.</p><br />
						<p class="fragment"><strong>Which normalized Laplacian to use?</strong> It seems that $L_{rw}$ is a better choice: The eigenvectors of $L_{rw}$ are cluster indicator vectors $\mathbb{1}_{A_i}$, while those of $L_{sym}$ are additionally multiplied with $D^{1/2}$, which might lead to undesired artifacts. Plus, there are no computational advantages for $L_{sym}$.<br />So as conclusion, $L_{rw}$ is the better choice.</p>
					</section>
				</section>
				<!--<section>-->
					<!--<h2>Appendices</h2>-->
				<!--</section>-->
				<section>
					<section>
						<h2>Toy example</h2>
					</section>
					<section>
						<h3>Toy example</h3>
						<p>This example data set consists of a random sample of 200 points $x_1,...,x_{200}\in\mathbb{R}$ drawn according to a mixture of four Gaussians.</p>
						<p class="fragment"><img data-src="imgs/figure1_1.png" style="padding:5px 0 5px 5px; float: right;" />This is a histogram of a sample drawn from this distribution (the $x$-axis represents the one-dimensional data space). The Gaussian similarity function was chosen, with $\sigma=1$. For similarity graph, both fully connected and $10$-nearest neighbor graphs are considered.</p>
						<p class="fragment">We're going to show the first eigenvalues and eigenvectors of $L$ and $L_{rw}$. In the eigenvalue plots we plot $i$ vs. $\lambda_i$. In the eigenvector plots of an eigenvector $u=(u_1,...,u_{200})'$ we plot $x_i$ vs. $u_i$ (where here, $x_i$ is simply a real number, so we can put it on $x$-axis).</p>
					</section>
					<section>
						<h3>Toy example - eigenvalues and eigenvectors</h3>
						<img data-src="imgs/figure1_2.png" />
						<p class="fragment">Here are the results based on the $10$-nearest neighbor graph. We can see that the first four eigenvalues are $0$, and the corresponding eigenvectors are cluster indicator vectors.</p>
						<p class="fragment">The clusters form disconnected parts in the $10$-nearest neighbor graph, and the eigenvectors are given by <a target="_blank" href="#spectrum_of_L">this</a> and <a href="#spectrum_of_L_and_L" target="_blank">this</a>, accordingly.</p>
					</section>
					<section>
						<h3>Toy example - eigenvalues and eigenvectors - cont.</h3>
						<img data-src="imgs/figure1_3.png" />
						<p class="fragment" style="font-size: 20px;">Here are the results based on the fully connected graph. As the Gaussian similarity function is always positive, this graph only consists of one connected component. Thus, eigenvalue 0 has multiplicity $1$, and the first eigenvector is the constant vector. The following eigenvectors carry the information about the clusters.</p>
						<p class="fragment" style="font-size: 20px;">For example, in the unnormalized case, if we threshold the 2nd vector at $0$, then the part below $0$ relate to clusters 1 and 2, and above $0$ relate to clusters 3 and 4, and so on.</p>
						<p class="fragment" style="font-size: 20px;">In all cases, the first 4 vectors carry all the info about the 4 clusters. And in all the cases - applying $k$-means on this 4 vectors easily detects the correct 4 clusters.</p>
					</section>
					<section>
						<h3>Toy example - choosing type of similarity graph</h3>
						<p style="width: 100%; float: left;"><img data-src="imgs/figure3_1.png" style="padding:5px 0 5px 5px; width: 290px; float: right;" />We take distribution on $\mathbb{R}^2$ with three clusters: two "moons" and a Gaussian. The density of the bottom moon is chosen to be larger than the one of the top moon.</p>
						<p style="width: 100%; float: left;" class="fragment"><img data-src="imgs/figure3_2.png" style="padding:5px 0 5px 5px; width: 290px; float: right;" />Here we can see that it is difficult to choose a useful parameter $\epsilon$. With $\epsilon=0.3$ as in the image, the points on the middle moon are already very tightly connected, while the points in the Gaussian are barely connected. This problem usually occur when the distance between data points is different between regions (named points "on different scales").</p>
					</section>
					<section>
						<h3>Toy example - choosing type of similarity graph - cont.</h3>
						<p style="width: 100%; float: left;"><img data-src="imgs/figure3_3.png" style="padding:5px 0 5px 5px; width: 290px; float: right;" />
						$k$-nearest neighbor, though, can connect points "on different scales". We can see that the points in the low-density Gaussian are connected with the high-density moon. But, the 2 high-density moons are clustered apart, as should be. This is a general property of $k$-nearest neighbor graphs - adding low-density region to a close high-density region, and seperating far away high-density regions.
						</p>
						<p style="width: 100%; float: left;" class="fragment"><img data-src="imgs/figure3_4.png" style="padding:5px 0 5px 5px; width: 290px; float: right;" />
							The mutual $k$-nearest neighbor graph connect points within regions of constant density, but does not connect regions of different densities with each other.
							So it does connect points "on different scales", but doesn't mix those scales with each other. So it seems particularly well-suited in detecting clusters of different densities.</p>
					</section>
					<section>
						<h3>Toy example - number of clusters - eigengap heuristic</h3>
						<img data-src="imgs/figure4.png" />
						<p>The first row shows the histogram of 3 samples. We construct the $10$-nearest graph and plot the eigenvalues of $L_{rw}$ of each sample (Results for $L$ are similar). The first data sample consists of 4 well seperated clusters, and we can see that the first 4 eigenvalues are $\sim 0$. Then the gap between the 4th and 5th eigenvalue is large, which according to the eigengap heuristic, indicates that the data set contains 4 clusters. In the other samples, the clusters are less pronounced, and the eigengap heuristic is less effective.</p>
					</section>
					<section>
						<h3>Toy example - consistencies issues</h3>
						<img data-src="imgs/figure1_3.png" style="width: 700px;"/>
						<p>In the <a href="#minimal_degree">Practical details</a> part we said that for unnormalized clustering, the way to ensure that the eigenvectors are useful, their eigenvalues need to be significantly lower than the graph minimal degree ("min-deg").</p>
						<p class="fragment">In the image up here and in the next slide, the dashed line is the min-deg, eigenvectors above it are blue stars, below are red diamonds. </p>
						<p class="fragment">We can see in the image, that the eigenvector corresponding the "red" eigenvalues are "useful" eigenvectors. When $\sigma=1$, the eigenvalues 2,3,4 are below the min-deg, and their vectors are meaningful. </p>
					</section>
					<section>
						<h3>Toy example - consistencies issues - cont.</h3>
						<img data-src="imgs/figure5.png" style="width: 700px;" />
						<p>Now we increase $\sigma$, and we can see as the eigenvalues tend to move toward the min-deg.</p>
						<p class="fragment">In case of $\sigma=2$, only first 3 values are below the min-deg, and in case when $\sigma=5$, only the first 2 values are below it.</p>
						<p class="fragment">We can also see, that as soon as the eigenvalue get closer to the min-deg, their eigenvector approximate a Dirac fucntion, and they are useless for clustering.</p>
					</section>
				</section>

				<section>
					<section>
						<h2>Conclusion</h2>
					</section>
					<section>
						<h3>Some final words</h3>
						<p>To summerize, main advantages of spectral clustering over other clustering methods are:</p>
						<ol>
							<li class="fragment">Spectral clustering does not make strong assumptions on the form of the clusters. (For example, $k$-means results in convex sets as clusters).</li>
							<li class="fragment">Spectral clustering can be implemented efficiently even for large data sets, as long as we make sure that the similarity graph is sparse.</li>
							<li class="fragment">Once the similarity graph is chosen, it is a linear problem to solve (no issues of getting stuck in local minimum or need to restart the algorithm with different parameters).</li>
						</ol>
						<p class="fragment">However, spectral clustering cannot serve as a "black box algorithm":</p>
						<ol>
							<li class="fragment">It is no trivial task to choose a good similarity graph.</li>
							<li class="fragment">Spectral clustering can be quite unstable under different choices of parameters for the neighborhood graph.</li>
						</ol>
						<p class="fragment">So spectral clustering can be considered a powerful tool that can produce good results, if applied with care.</p>
					</section>
				</section>
				<!--<section>
					<section>
						<h2>Mathematical Proofs</h2>
					</section>
					<section id="proof1">
						<h3>Proof of the unnormalized graph Laplacian Properties</h3>
						<a href="#unnormalized_lapl">back</a>
					</section>
					<section id="proof2">
						<h3>Proof of the spectrum of L</h3>
						<a href="#spectrum_of_L">back</a>
					</section>
					<section id="proof3">
						<h3>Proof of the properties of L and L</h3>
						<a href="#prop_of_L_and_L">back</a>
					</section>
					<section id="proof4">
						<h3>Proof of the spectrum of L and L</h3>
						<a href="#spectrum_of_L_and_L">back</a>
					</section>
					<section id="proof5">
						<h3>Proof of the spectrum of L and L</h3>
						<a href="#rw_first_prop">back</a>
					</section>
				</section>-->
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// More info https://github.com/ilanp13/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				math: {
					mathjax: 'plugin/MathJax/MathJax.js',
					config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				},
				// More info https://github.com/ilanp13/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
