<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Spectral Clustering</title>

		<meta name="description" content="A presentation on spectral clustering for applied mathematics seminar, May 2016">
		<meta name="author" content="Ilan Peretz">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/simple.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Spectral Clustering</h3>
					<h6>By Ilan Peretz</h6>
				</section>

				<section>
					<section>
						<h3>What does "Spectral Clustering" means anyway?</h3>
					</section>

					<section>
						<h3>What is Clustering?</h3>
						<ul class="fragment">
							<li>
								<q>&ldquo;Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).&rdquo;</q><p>(-- <a href="https://en.wikipedia.org/wiki/Cluster_analysis" target="_blank">Wikipedia</a>)</p>
							</li>
							<li class="fragment">
								<p>So the goal is the same as we've seen before in this room: creating an algorithm that upon receiving a sample of sort, will be able to decide and know in which group to put that sample.</p>
							</li>
						</ul>
					</section>
					<section>
						<h3>What is the meaning of Spectral?</h3>
						<ul>
							<li class="fragment">
								<p>Spectral is coming from the terminology <b>Spectral Graph Theory</b>.</p>
							</li>
							<li class="fragment">
								<span><q>&ldquo;In mathematics, <b>spectral graph theory</b> is the study of properties of a graph in relationship to the characteristic polynomial, eigenvalues, and eigenvectors of matrices associated to the graph, such as its adjacency matrix or Laplacian matrix.&rdquo;</q>(-- <a href="https://en.wikipedia.org/wiki/Spectral_graph_theory" target="_blank">Wikipedia</a>)</span>
							</li>
							<li class="fragment">
								<p>So in this algorithm, we will use Similarity Graphs and their Adjacency Matrix, which we will use to create the Laplacian Matrix of the graph, or the Graph Laplacian.</p>
							</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h3>Lets start with some general definitions:</h3>
					</section>
					<section>
						<h3>Similarity Graphs</h3>
						<ul>
							<li class="fragment">
								So, we have a set of data points $ x_1, ..., x_n $ and some similarity parameter $ s_{ij} \geq 0 $ between all pairs of points $ x_i $ and $ x_j $. One way to represent the data is in the form of similarity graph $ G=(V,E) $:
								<ul>
									<li class="fragment">Each vertex $ v_i \in V$ represents a data point $ x_i $.</li>
									<li class="fragment">Two vertices $ s_i $ and $ s_j $ are connected if $ s_{ij} \geq a $, when $ a $ is 0 or other threshold, and then the edge $e=(s_i,s_j) \in E$ has weight $s_{ij}$. </li>
								</ul>
							</li>
						</ul>
					</section>
					<section>
						<h3>Similarity Graphs - continue</h3>
						<p>So now we can reformulate the problem of clustering to the problem of finding a partition of the graph $G$ such that the edges between the different groups in the partition have very low (if at all) weights, and the weights of edges inside a group have a very high weights.</p>
					</section>
					<section>
						<h3>Graph Adjacency Matrix</h3>
						<ul>
							<li>
								Let $G=(V,E)$ be an undirected graph with vertex set $V=\{v_1,...,v_n\}$.
								We also assume that the graph is weighted with non-negative weights $w_{ij}$.
							</li>
							<li class="fragment">
								The weighted <i>adjacency matrix</i> of the graph is the matrix $W=(w_{ij})_{i,j=1,...,n}$.
							</li>
							<li class="fragment">
								Note:
								<ol>
									<li>
										$w_{ij}=0$ means that $v_i$ and $v_j$ are not connected by an edge.
									</li>
									<li class="fragment">
										$w_{ij}=w_{ji}$ as $G$ is undirected.
									</li>
								</ol>
							</li>
						</ul>
					</section>
					<section>
						<h3>Degree Matrix</h3>
						<ul>
							<li>
								The degree of $v_i \in V$ is defined as: \[d_i=\sum_{j=1}^n w_{ij}\]
							</li>
							<li class="fragment">
								The <i>degree matrix</i> is the diagonal matrix, when the cells on the diagonal are $d_1,...,d_n$. In other words, every value on the diagonal of $D$ is the sum of the cells of the same column in $W$.
							</li>
						</ul>
					</section>
					<section>
						<h3>Some graph partition notations</h3>
						<ul>
							<li>If $A \subset V$ is a subset of vertices, then $\bar{A}$ is the complement $V \setminus A$.</li>
							<li class="fragment">
								Vector indicator : $\mathbb{1}_A=(f_1,...,f_n)' \in \{0,1\}$, when $f_i=1$ if $v_i \in A$ and $f_i=0$ if $v_i \notin A$. We'll shortify it to $i \in A$ for $\{i \mid v_i \in A\}$.
							</li>
							<li class="fragment">
								For any two sets $A,B$(not necessarily disjoint) we can define:
								\[
								W(A,B) := \sum_{i \in A,j \in B}W_{ij}
								\]
							</li>
						</ul>
					</section>
					<section>
						<h3>Some graph partition notations - continue</h3>
						<ul>
							<li>
								There are two options to measure the "size" of subset $A \subset V $ :
								<ol>
									<li class="fragment">
										$|A| := \#\{v_i \in A\}$ (number of vertices in $A$).
									</li>
									<li class="fragment">
										$vol(A) := \sum_{i \in A}d_i$
										(Summing over the weights of all the edges attached to vertices in $A$).
									</li>
								</ol>
							</li>
						</ul>
					</section>
					<section>
						<h3>graph connectivity</h3>
						<ul>
							<li>
								$A \subset V$ is <i>connected</i> if any two vertices in $A$ can be joined by path, that does not leave $A$ (every vertex on the path is in $A$).
							</li>
							<li class="fragment">
								$A \subset V$ is called <i>connected component</i> if it is connected and there are no connections between vertices in $A$ and $\bar{A}$.
							</li>
							<li class="fragment">
								The nonempty set $ A_1,...,A_k $ form a <i>partition of $G$</i> if $A_i \cap A_j = \emptyset $ ($\forall i,j \in [1,k], i\neq j$) and $ A_1 \cup \cdot\cdot\cdot \cup A_k= V$.
							</li>
						</ul>
					</section>
					<section>
						<h3>Types of similarity graphs</h3>
						<p>We'll mention the popular constructions to transform our data points into graph:</p>
						<ul>
							<li class="fragment">
								<strong>The $\epsilon$-neighborhood graph:</strong> Connect all points whose pairwise similarities/distances are smaller than $\epsilon$.
							</li>
							<li class="fragment">
								<strong>$k$-nearest neighbor graphs:</strong> Connect vertex $v_i$ with vertex $v_j$ if $v_j$ is among the $k$-nearest neighbors of $v_i$. The result here is a directed graph, so there are two undirected versions of it:
								<ul>
									<li class="fragment">
										<i>$k$-nearest neighbor graphs</i> - connect $v_i$ and $v_j$ if $v_i$ is $k$-neighbor of $v_j$ <strong>or</strong> if $v_j$ is $k$-neighbor of $v_i$.
									</li>
									<li class="fragment">
										<i>Mutual $k$-nearest neighbor graphs</i> - connect $v_i$ and $v_j$ if $v_i$ is $k$-neighbor of $v_j$ <strong>and</strong> if $v_j$ is $k$-neighbor of $v_i$.
									</li>
								</ul>
							</li>
						</ul>
					</section>
					<section>
						<h3>Types of similarity graphs - continue</h3>
						<ul>
							<li class="fragment">
								<strong>The fully connected graph:</strong> Connect all points with positive similarity between them, which is the weight of the edge. This type is only useful if the similarity function itself models local neighborhoods. As example we can take the Gaussian similarity function:
								\[
								s(x_i,x_j)=e^{{-\|x_i-x_j\|^2}/{2\sigma^2}}
								\]
								where $\sigma$ controls the width of the neighborhood (similar to the $\epsilon$ before).
							</li>
						</ul>
						<p>All this graphs are used in spectral clustering. There is no info on how the choice of method influence the results.</p>
					</section>
				</section>
				<section>
					<section>
						<h3>Graph Laplacians</h3>
						<h4>And their (basic) properties</h4>
					</section>
					<section>
						<h3>Graph Laplacians</h3>
						<p>As we mentioned earlier, the concept of Spectral Clustering is coming from the field of <i>spectral graph theory</i>, in which graph Laplacians matrices are the primary tool. So they will be primary tool in spectral clustering as well.</p>
						<p>We are now going to define the graph Laplacians and mention their most important properties.</p>
						<small>Note: There is no unique convention which matrix exactly is called "graph Laplacians". <br />Mostly, every author gives is own version of it.</small>
					</section>
					<section>
						<h3>Graph Laplacians</h3>
						<h4>Pre-assumptions:</h4>
						<ul>
							<li class="fragment">
								Graph $G$ is an undirected, weighted graph with weight matrix $W$, where $w_{ij}=w_{ji}\geq0$.
							</li>
							<li class="fragment">
								Eigenvectors of a matrix not necessarily have to be normalized. (For example, the vector $e_i$ and the vector $ae_i$ for some $a\neq0$ will be considered the same vector).
							</li>
							<li class="fragment">
								Eigenvalues will be ordered increasingly, respecting multiplicities.
							</li>
							<li class="fragment">
								By "the first $k$ eigenvectors" we refer to the eigenvectors corresponding to the $k$ smallest eigenvalues.
							</li>
						</ul>
					</section>
					<section>
						<h3>Unnormalized Graph Laplacians</h3>
						<p>Considering D - the <i>degree matrix</i> of $G$, and $W$ - the weight matrix of $G$, we get the <strong><i>unnormalized graph Laplacian</i></strong>:
						\[
							L=D-W
							\]
						</p>
						<br />
						<br />
						<small class="fragment">Note: self edges, which "sits" on the diagonal of the adjacency matrix $W$, do not change the graph Laplacians.</small>
					</section>
					<section id="unnormalized_lapl">
						<h3>Unnormalized Graph Laplacians</h3>
						<h4>Lets introduce some important properties of $L$:</h4>
						<ol>
							<li class="fragment">
								For every vector $f \in \mathbb{R}^n$ we have:
								$f'Lf=\frac{1}{2}\sum_{i,j=1}^nw_{ij}(f_i-f_j)^2$
							</li>
							<li class="fragment">
								$L$ is symmetric and positive semi-definite.
							</li>
							<li class="fragment">
								The smallest eigenvalue of $L$ is $0$, the corresponding eigenvector is the constant one vector $\mathbb{1}$.
							</li>
							<li class="fragment">
								$L$ has $n$ non-negative, real-valued eigenvalues $0=\lambda_1\leq\lambda_2\leq\cdot\cdot\cdot\leq\lambda_n$.
							</li>
						</ol>
						<br />
						<br />
						<small class="fragment">The <i>proof</i> of the 4 properties can be found in the appendix <a href="#proof1">here</a>.</small>
					</section>
					<section id="spectrum_of_L">
						<h3>Unnormalized Graph Laplacians</h3>
						<h4>Number of connected components and the spectrum of $L$</h4>
						<p>Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue $0$ of $L$ equals the number of connected components $A_1,...,A_k$ in the graph. The eigenspace of eigenvalue $0$ is spanned by the indicator vectors $\mathbb{1}_{A_1},...,\mathbb{1}_{A_k}$ of those components.</p>
						<br />
						<br />
						<small class="fragment">The <i>proof</i> can be found in the appendix <a href="#proof2">here</a>.</small>
					</section>
					<section>
						<h3>The normalized graph Laplacians</h3>
						<p>There are two matrices which are called normalized graph Laplacians in the literature. Both of them are closely related to each other:</p>
						<p class="fragment">
							A symmetric normalized graph Laplacian:
						\[
							L_{sym}:=D^{-1/2}LD^{-1/2}=I-D^{-1/2}WD^{-1/2}
							\]</p>
						<p class="fragment">
							A Random Walk like normalized graph Laplacian:
						\[
							L_{rw}:=D^{-1}L=I-d^{-1}W
							\]</p>
					</section>
					<section id="prop_of_L_and_L_0">
						<h3>The normalized graph Laplacians</h3>
						<h4>Properties of $L_{sym}$ and $L_{rw}$</h4>
						<ol>
							<li class="fragment">
								For every vector $f \in \mathbb{R}^n$ we have:
								\[
								f'L_{sym}f=\frac{1}{2}\sum_{i,j=1}^nw_{ij}\Bigg(\frac{f_i}{\sqrt{d_i}}-\frac{f_j}{\sqrt{d_j}}\Bigg)^2
								\]
							</li>
							<li class="fragment">
								$\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $\lambda$ is an eigenvalue of $L_{sym}$ with eigenvector $w=D^{1/2}u$.
							</li>
							<li class="fragment">
								$\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $\lambda$ and $u$ solve the generalized eigen-problem $Lu=\lambda Du$.
							</li>
						</ol>
					</section>
					<section id="prop_of_L_and_L">
						<h3>The normalized graph Laplacians</h3>
						<h4>Properties of $L_{sym}$ and $L_{rw}$ - continue</h4>
						<ol start="4">
							<li class="fragment">
								$0$ is an eignevalue of $L_{rw}$ with the constant one vector $\mathbb{1}$ as eigenvector. $0$ is an eigenvalue of $L_{sym}$ with eigenvector $D^{1/2}\mathbb{1}$.
							</li>
							<li class="fragment">
								$L_{sym}$ and $L_{rw}$ are positive semi-definite and have $n$ non-negative real-valued eigenvalues $0=\lambda_1\leq\lambda_2\leq\cdot\cdot\cdot\leq\lambda_n$.
							</li>
						</ol>
						<br />
						<br />
						<small class="fragment">The <i>proof</i> of the 5 properties can be found in the appendix <a href="#proof3">here</a>.</small>
					</section>
					<section id="spectrum_of_L_and_L">
						<h3>The normalized Graph Laplacians</h3>
						<h4>Number of connected components, the spectrum of $L_{sym}$ & $L_{rw}$</h4>
						<p>Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue $0$ of both $L_{rw}$ and $L_{sym}$ equals the number of connected components $A_1,...,A_k$ in the graph. <br />For $L_{rw}$, the eigenspace of $0$ is spanned by the indicator vectors $\mathbb{1}_{A_1},...,\mathbb{1}_{A_k}$ of those components. <br />For $L_{sym}$, the eigenspace of $0$ is spanned by the vectors $D^{1/2}\mathbb{1}_{A_i}$.</p>
						<br />
						<br />
						<small class="fragment">The <i>proof</i> can be found in the appendix <a href="#proof4">here</a>.</small>
					</section>
				</section>
				<section>
					<section>
						<h3>Spectral Clustering Algorithms</h3>
					</section>
					<section>
						<h3>Spectral Clustering Algorithms</h3>
						<p>In this part we going to see the most common spectral clustering algorithms.</p>
						<p class="fragment">Some assumptions before we start:</p>
						<ol>
							<li class="fragment">Our data consists of $n$ "points" $x_1,...,x_n$ which can be arbitrary objects.</li>
							<li class="fragment">The pairwise similarities $s_{ij}=s(x_i,x_j)$ is defined by some similarity function that is symmetric and non-negative.</li>
							<li class="fragment">The similarity matrix is defined as $S=(s_{ij})_{i,j=1,...,n}$</li>
							<li class="fragment">As mentioned before, there is no info about which way of constructing similarity graph is better. So the method of construction is (for now) not an issue.</li>
						</ol>
					</section>
					<section>
						<h3>Unnormalized spectral clustering algorithm</h3>
						<p><strong>Input:</strong> Similarity matrix $S \in \mathbb{R}^{n\times n}$, $k$ of clusters to construct.</p>
						<ul style="font-size: 30px;">
							<li class="fragment">Construct a similarity graph, get $W$ - its weighted adjacency matrix.</li>
							<li class="fragment">Compute the unnormalized Laplacian $L$.</li>
							<li class="fragment"><strong>Compute the first $k$ eigenvectors $u_1,...,u_k$ of $L$.</strong></li>
							<li class="fragment">Let $U \in \mathbb{R}^{n \times k}$ be the matrix with the vectors $u_1,...,u_k$ as columns.</li>
							<li class="fragment">For $i=1,...,n$, let $y_i \in \mathbb{R}^k$ be the $i$-th vector row of $U$.</li>
							<li class="fragment">Cluster the points $(y_i)_{i=1,...,n} \in \mathbb{R}^k$ with the $k$-means algorithm into clusters $C_1,...,C_k$.</li>
						</ul>
						<p class="fragment"><strong>Output:</strong> Clusters $A_1,...,A_k$ with $A_i=\{j \mid y_j \in C_j\}$</p>
					</section>
					<section>
						<h3>Normalized spectral clustering algorithm</h3>
						<h6>By $L_{rw}$, according to <i>Shi and Malik (2000)</i></h6>
						<p><strong>Input:</strong> Similarity matrix $S \in \mathbb{R}^{n\times n}$, $k$ of clusters to construct.</p>
						<ul style="font-size: 30px;">
							<li class="fragment">Construct a similarity graph, get $W$ - its weighted adjacency matrix.</li>
							<li class="fragment">Compute the unnormalized Laplacian $L$.</li>
							<li class="fragment"><strong>Compute the first $k$ generalized eigenvectors $u_1,...,u_k$ of the generalized eigenproblem $Lu=\lambda Du$.</strong></li>
							<li class="fragment">Let $U \in \mathbb{R}^{n \times k}$ be the matrix with the vectors $u_1,...,u_k$ as columns.</li>
							<li class="fragment">For $i=1,...,n$, let $y_i \in \mathbb{R}^k$ be the $i$-th vector row of $U$.</li>
							<li class="fragment">Cluster the points $(y_i)_{i=1,...,n} \in \mathbb{R}^k$ with the $k$-means algorithm into clusters $C_1,...,C_k$.</li>
						</ul>
						<p class="fragment"><strong>Output:</strong> Clusters $A_1,...,A_k$ with $A_i=\{j \mid y_j \in C_j\}$</p>
					</section>
					<section>
						<h3>Normalized spectral clustering algorithm</h3>
						<h6>By $L_{sym}$, according to <i>Ng, Jordan and Weiss (2002)</i></h6>
						<p><strong>Input:</strong> Similarity matrix $S \in \mathbb{R}^{n\times n}$, $k$ of clusters to construct.</p>
						<ul style="font-size: 30px;">
							<li class="fragment">Construct a similarity graph, get $W$ - its weighted adjacency matrix.</li>
							<li class="fragment">Compute the normalized Laplacian $L_{sym}$.</li>
							<li class="fragment"><strong>Compute the first $k$ eigenvectors $u_1,...,u_k$ of $L_{sym}$.</strong></li>
							<li class="fragment">Let $U \in \mathbb{R}^{n \times k}$ be the matrix with the vectors $u_1,...,u_k$ as columns.</li>
							<li class="fragment"><strong>Form the matrix $T \in \mathbb{R}^{n \times k}$ from $U$ by normalizing the rows to norm 1: $t_{ij}=u_{ij}/(\sum_ku_{ik}^2)^{1/2}$.</strong></li>
							<li class="fragment">For $i=1,...,n$, let $y_i \in \mathbb{R}^k$ be the $i$-th vector row of $T$.</li>
							<li class="fragment">Cluster the points $(y_i)_{i=1,...,n} \in \mathbb{R}^k$ with the $k$-means algorithm into clusters $C_1,...,C_k$.</li>
						</ul>
						<p class="fragment"><strong>Output:</strong> Clusters $A_1,...,A_k$ with $A_i=\{j \mid y_j \in C_j\}$</p>
					</section>
					<section>
						<h3>spectral Clustering Algorithm</h3>
						<h4>Some notions:</h4>
						<ul>
							<li class="fragment">All three algorithms looks rather similar, apart from the fact that they use three different graph Laplacians.</li>
							<li class="fragment">The main trick in all three is to change the representation of the abstract data points $x_i$ to points $y_i \in \mathbb{R}^k$.</li>
							<li class="fragment">Due to the properties of the graph Laplacians, this change of representation of the points enhances the cluster-properties in the data - making the detection of clusters trivial task.</li>
							<li class="fragment">In our algorithms, we did it with the simple $k$-means clustering algorithm - with no difficulties.</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h3>Graph cut point of view</h3>
					</section>
					<section>
						<h3>Graph cut point of view</h3>
						<p>As we know, clustering is to seperate points in different groups according to their similarities.</p>
						<p class="fragment">If the data is given in form of a similarity graph, we can restate the clustering problem as <i>graph partitioning</i> problem, such that the edges between different groups of the graph have a very low weight, and the edges within a group have high weight.</p>
					</section>
					<section>
						<h3>Graph cut point of view - mincut</h3>
						<p>The simplest and most direct way to construct a partition of a graph is to solve the <strong>mincut</strong> problem: For a given number $k$ of subsets - choose a partition $A_1,...,A_k$ that minimizes
						\[
							\mbox{cut}(A_1,...,A_k):=\frac{1}{2}\sum_{i=1}^kW(A_i,\bar{A_i})
							\]
							<br />Here, we use the notation $W(A,B):=\sum_{i \in A,j \in B}w_{ij}$ that we seen before, and $\bar{A}$ for the complement of $A$.
						</p>
					</section>
					<section>
						<h3>Mincut - continue</h3>
						<p>For $k=2$, mincut is relatively easy problem to solve, even efficiently. But, in practice, sometimes the result is not what we want: In many cases, mincut simply separates one individual vertex from the rest.</p>
						<p class="fragment">One way to solve it, is to explicitly request that the sets $A_1,...,A_k$ are "reasonably large".</p>
						<p class="fragment">The two most common functions to encode this are RatioCut and the normalized cut Ncut.</p>
					</section>
					<section>
						<h3>RatioCut & Ncut - continue</h3>
						<p>
							\[
							\mbox{RatioCut}(A_1,...,A_k):=\frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\bar{A_i})}{|A_i|}=\sum_{i=1}^k\frac{\mbox{cut}(A_i,\bar{A_i})}{|A_i|} \\
							\mbox{Ncut}(A_1,...,A_k):=\frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\bar{A_i})}{\mbox{vol}(A)}=\sum_{i=1}^k\frac{\mbox{cut}(A_i,\bar{A_i})}{\mbox{vol}(A)}
							\]
						</p>
						<p class="fragment">The difference between RatioCut and Ncut is how they defined the size of a subset $A_i$: <br />RatioCut define it as the the number of vertices in $A_i$: $|A_i|$.<br />Ncut define it as the weights of the edges in $A_i$: $\mbox{vol}(A)$.</p>
					</section>
					<section>
						<h3>RatioCut & Ncut - continue</h3>
						<p>As easily seen in the formulas of RatioCut and Ncut, both of them are minimized if the respective sizes of the clusters $A_i$ are large "together", or in other words - balanced.</p>
						<p class="fragment">This added condition (balanced clusters) makes the previously simple problem to solve now a NP hard.</p>
						<p class="fragment">Spectral clustering is a way to solve relaxed versions of those problems.</p>
					</section>
					<section>
						<h3>Approximating RatioCut for $k=2$</h3>
						<p>Our goal is to solve the optimization problem $\min_{A\subset V}{\mbox{RatioCut}(A,\bar{A})}$. Lets rewrite the problem in a more convenient way.</p>
						<p class="fragment">For given subset $A \subset V$ we defined $f=(f_1,...,f_n)'\in \mathbb{R}^n$ such that:
						\[
							f_i=
							\left\{
							\begin{array}{ll}
							\sqrt{|\bar{A}|/|A|}  & \mbox{if } v_i \in A \\
							-\sqrt{|A|/|\bar{A}|}  & \mbox{if } v_i \in \bar{A}
							\end{array}
							\right.
						\]</p>
					</section>
					<section>
						<h3>Apprx. RatioCut for $k=2$ - continue</h3>
						<p>Now we can rewrite the RatioCut function with the unnormalized graph Laplacian:</p>
						<small>\[
						f'Lf=\frac{1}{2}\sum_{i,j=1}^nw_{ij}(f_i-f_j)^2 \\
						=\frac{1}{2}\sum_{i\in A,j\in \bar{A}}w_{ij}\Bigg(\sqrt{\frac{\bar{A}}{A}}+\sqrt{\frac{A}{\bar{A}}}\Bigg)^2 +
						\frac{1}{2}\sum_{i\in \bar{A},j\in A}w_{ij}\Bigg(-\sqrt{\frac{\bar{A}}{A}}-\sqrt{\frac{A}{\bar{A}}}\Bigg)^2 \\
						=\mbox{cut}(A,\bar{A})
						\]</small>
					</section>
				</section>
				<section>
					<section>
						<h3>toy example - starts in pages 8-9</h3>
					</section>
				</section>
				<section>
					<section>
						<h3>Appendices</h3>
					</section>
					<section id="proof1">
						<h3>Proof of the unnormalized graph Laplacian Properties</h3>
						<a href="#unnormalized_lapl">back</a>
					</section>
					<section id="proof2">
						<h3>Proof of the spectrum of L</h3>
						<a href="#spectrum_of_L">back</a>
					</section>
					<section id="proof3">
						<h3>Proof of the properties of L and L</h3>
						<a href="#prop_of_L_and_L">back</a>
					</section>
					<section id="proof4">
						<h3>Proof of the spectrum of L and L</h3>
						<a href="#spectrum_of_L_and_L">back</a>
					</section>
				</section>
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// More info https://github.com/ilanp13/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				math: {
					mathjax: 'plugin/MathJax/MathJax.js',
					config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				},
				// More info https://github.com/ilanp13/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
