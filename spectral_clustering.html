<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Spectral Clustering</title>

		<meta name="description" content="A presentation on spectral clustering for applied mathematics seminar, May 2016">
		<meta name="author" content="Ilan Peretz">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/simple.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Spectral Clustering</h3>
					<h6>By Ilan Peretz</h6>
				</section>

				<section>
					<section>
						<h3>What does "Spectral Clustering" means anyway?</h3>
					</section>

					<section>
						<h3>What is Clustering?</h3>
						<ul class="fragment">
							<li>
								<q>&ldquo;Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).&rdquo;</q><p>(-- <a href="https://en.wikipedia.org/wiki/Cluster_analysis" target="_blank">Wikipedia</a>)</p>
							</li>
							<li class="fragment">
								<p>So the goal is the same as we've seen before in this room: creating an algorithm that upon receiving a sample of sort, will be able to decide and know in which group to put that sample.</p>
							</li>
						</ul>
					</section>
					<section>
						<h3>What is the meaning of Spectral?</h3>
						<ul>
							<li class="fragment">
								<p>Spectral is coming from the terminology <b>Spectral Graph Theory</b>.</p>
							</li>
							<li class="fragment">
								<span><q>&ldquo;In mathematics, <b>spectral graph theory</b> is the study of properties of a graph in relationship to the characteristic polynomial, eigenvalues, and eigenvectors of matrices associated to the graph, such as its adjacency matrix or Laplacian matrix.&rdquo;</q>(-- <a href="https://en.wikipedia.org/wiki/Spectral_graph_theory" target="_blank">Wikipedia</a>)</span>
							</li>
							<li class="fragment">
								<p>So in this algorithm, we will use Similarity Graphs and their Adjacency Matrix, which we will use to create the Laplacian Matrix of the graph, or the Graph Laplacian.</p>
							</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h3>Lets start with some general definitions:</h3>
					</section>
					<section>
						<h3>Similarity Graphs</h3>
						<ul>
							<li class="fragment">
								So, we have a set of data points $ x_1, ..., x_n $ and some similarity parameter $ s_{ij} \geq 0 $ between all pairs of points $ x_i $ and $ x_j $. One way to represent the data is in the form of similarity graph $ G=(V,E) $:
								<ul>
									<li class="fragment">Each vertex $ v_i \in V$ represents a data point $ x_i $.</li>
									<li class="fragment">Two vertices $ s_i $ and $ s_j $ are connected if $ s_{ij} \geq a $, when $ a $ is 0 or other threshold, and then the edge $e=(s_i,s_j) \in E$ has weight $s_{ij}$. </li>
								</ul>
							</li>
						</ul>
					</section>
					<section>
						<h3>Similarity Graphs - continue</h3>
						<p>So now we can reformulate the problem of clustering to the problem of finding a partition of the graph $G$ such that the edges between the different groups in the partition have very low (if at all) weights, and the weights of edges inside a group have a very high weights.</p>
					</section>
					<section>
						<h3>Graph Adjacency Matrix</h3>
						<ul>
							<li>
								Let $G=(V,E)$ be an undirected graph with vertex set $V=\{v_1,...,v_n\}$.
								We also assume that the graph is weighted with non-negative weights $w_{ij}$.
							</li>
							<li class="fragment">
								The weighted <i>adjacency matrix</i> of the graph is the matrix $W=(w_{ij})_{i,j=1,...,n}$.
							</li>
							<li class="fragment">
								Note:
								<ol>
									<li>
										$w_{ij}=0$ means that $v_i$ and $v_j$ are not connected by an edge.
									</li>
									<li class="fragment">
										$w_{ij}=w_{ji}$ as $G$ is undirected.
									</li>
								</ol>
							</li>
						</ul>
					</section>
					<section>
						<h3>Degree Matrix</h3>
						<ul>
							<li>
								The degree of $v_i \in V$ is defined as: \[d_i=\sum_{j=1}^n w_{ij}\]
							</li>
							<li class="fragment">
								The <i>degree matrix</i> is the diagonal matrix, when the cells on the diagonal are $d_1,...,d_n$. In other words, every value on the diagonal of $D$ is the sum of the cells of the same column in $W$.
							</li>
						</ul>
					</section>
					<section>
						<h3>Some graph partition notations</h3>
						<ul>
							<li>If $A \subset V$ is a subset of vertices, then $\bar{A}$ is the complement $V \setminus A$.</li>
							<li class="fragment">
								Vector indicator : $\mathbb{1}_A=(f_1,...,f_n)' \in \{0,1\}$, when $f_i=1$ if $v_i \in A$ and $f_i=0$ if $v_i \notin A$. We'll shortify it to $i \in A$ for $\{i \mid v_i \in A\}$.
							</li>
							<li class="fragment">
								For any two sets $A,B$(not necessarily disjoint) we can define:
								\[
								W(A,B) := \sum_{i \in A,j \in B}W_{ij}
								\]
							</li>
						</ul>
					</section>
					<section>
						<h3>Some graph partition notations - continue</h3>
						<ul>
							<li>
								There are two options to measure the "size" of subset $A \subset V $ :
								<ol>
									<li class="fragment">
										$|A| := \#\{v_i \in A\}$ (number of vertices in $A$).
									</li>
									<li class="fragment">
										$vol(A) := \sum_{i \in A}d_i$
										(Summing over the weights of all the edges attached to vertices in $A$).
									</li>
								</ol>
							</li>
						</ul>
					</section>
					<section>
						<h3>graph connectivity</h3>
						<ul>
							<li>
								$A \subset V$ is <i>connected</i> if any two vertices in $A$ can be joined by path, that does not leave $A$ (every vertex on the path is in $A$).
							</li>
							<li class="fragment">
								$A \subset V$ is called <i>connected component</i> if it is connected and there are no connections between vertices in $A$ and $\bar{A}$.
							</li>
							<li class="fragment">
								The nonempty set $ A_1,...,A_k $ form a <i>partition of $G$</i> if $A_i \cap A_j = \emptyset $ ($\forall i,j \in [1,k], i\neq j$) and $ A_1 \cup \cdot\cdot\cdot \cup A_k= V$.
							</li>
						</ul>
					</section>
					<section>
						<h3>Types of similarity graphs</h3>
						<p>We'll mention the popular constructions to transform our data points into graph:</p>
						<ul>
							<li class="fragment">
								<strong>The $\epsilon$-neighborhood graph:</strong> Connect all points whose pairwise similarities/distances are smaller than $\epsilon$.
							</li>
							<li class="fragment">
								<strong>$k$-nearest neighbor graphs:</strong> Connect vertex $v_i$ with vertex $v_j$ if $v_j$ is among the $k$-nearest neighbors of $v_i$. The result here is a directed graph, so there are two undirected versions of it:
								<ul>
									<li class="fragment">
										<i>$k$-nearest neighbor graphs</i> - connect $v_i$ and $v_j$ if $v_i$ is $k$-neighbor of $v_j$ <strong>or</strong> if $v_j$ is $k$-neighbor of $v_i$.
									</li>
									<li class="fragment">
										<i>Mutual $k$-nearest neighbor graphs</i> - connect $v_i$ and $v_j$ if $v_i$ is $k$-neighbor of $v_j$ <strong>and</strong> if $v_j$ is $k$-neighbor of $v_i$.
									</li>
								</ul>
							</li>
						</ul>
					</section>
					<section>
						<h3>Types of similarity graphs - continue</h3>
						<ul>
							<li class="fragment">
								<strong>The fully connected graph:</strong> Connect all points with positive similarity between them, which is the weight of the edge. This type is only useful if the similarity function itself models local neighborhoods. As example we can take the Gaussian similarity function:
								\[
								s(x_i,x_j)=e^{{-\|x_i-x_j\|^2}/{2\sigma^2}}
								\]
								where $\sigma$ controls the width of the neighborhood (similar to the $\epsilon$ before).
							</li>
						</ul>
						<p>All this graphs are used in spectral clustering. There is no info on how the choice of method influence the results.</p>
					</section>
				</section>
				<section>
					<section>
						<h3>Graph Laplacians</h3>
						<h4>And their (basic) properties</h4>
					</section>
					<section>
						<h3>Graph Laplacians</h3>
						<p>As we mentioned earlier, the concept of Spectral Clustering is coming from the field of <i>spectral graph theory</i>, in which graph Laplacians matrices are the primary tool. So they will be primary tool in spectral clustering as well.</p>
						<p>We are now going to define the graph Laplacians and mention their most important properties.</p>
						<small>Note: There is no unique convention which matrix exactly is called "graph Laplacians". <br />Mostly, every author gives is own version of it.</small>
					</section>
					<section>
						<h3>Graph Laplacians</h3>
						<h4>Pre-assumptions:</h4>
						<ul>
							<li class="fragment">
								Graph $G$ is an undirected, weighted graph with weight matrix $W$, where $w_{ij}=w_{ji}\geq0$.
							</li>
							<li class="fragment">
								Eigenvectors of a matrix not necessarily have to be normalized. (For example, the vector $e_i$ and the vector $ae_i$ for some $a\neq0$ will be considered the same vector).
							</li>
							<li class="fragment">
								Eigenvalues will be ordered increasingly, respecting multiplicities.
							</li>
							<li class="fragment">
								By "the first $k$ eigenvectors" we refer to the eigenvectors corresponding to the $k$ smallest eigenvalues.
							</li>
						</ul>
					</section>
					<section>
						<h3>Unnormalized Graph Laplacians</h3>
						<p>Considering D - the <i>degree matrix</i> of $G$, and $W$ - the weight matrix of $G$, we get the <strong><i>unnormalized graph Laplacian</i></strong>:
						\[
							L=D-W
							\]
						</p>
						<br />
						<br />
						<small class="fragment">Note: self edges, which "sits" on the diagonal of the adjacency matrix $W$, do not change the graph Laplacians.</small>
					</section>
					<section id="unnormalized_lapl">
						<h3>Unnormalized Graph Laplacians</h3>
						<h4>Lets introduce some important properties of $L$:</h4>
						<ol>
							<li class="fragment">
								For every vector $f \in \mathbb{R}^n$ we have:
								$f'Lf=\frac{1}{2}\sum_{i,j=1}^nw_{ij}(f_i-f_j)^2$
							</li>
							<li class="fragment">
								$L$ is symmetric and positive semi-definite.
							</li>
							<li class="fragment">
								The smallest eigenvalue of $L$ is $0$, the corresponding eigenvector is the constant one vector $\mathbb{1}$.
							</li>
							<li class="fragment">
								$L$ has $n$ non-negative, real-valued eigenvalues $0=\lambda_1\leq\lambda_2\leq\cdot\cdot\cdot\leq\lambda_n$.
							</li>
						</ol>
						<br />
						<br />
						<small class="fragment">The <i>proof</i> of the 4 properties can be found in the appendix <a href="#proof1">here</a>.</small>
					</section>
					<section id="spectrum_of_L">
						<h3>Unnormalized Graph Laplacians</h3>
						<h4>Number of connected components and the spectrum of $L$</h4>
						<p>Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue $0$ of $L$ equals the number of connected components $A_1,...,A_k$ in the graph. The eigenspace of eigenvalue $0$ is spanned by the indicator vectors $\mathbb{1}_{A_1},...,\mathbb{1}_{A_k}$ of those components.</p>
						<br />
						<br />
						<small class="fragment">The <i>proof</i> can be found in the appendix <a href="#proof2">here</a>.</small>
					</section>
					<section>
						<h3>The normalized graph Laplacians</h3>
						<p>There are two matrices which are called normalized graph Laplacians in the literature. Both of them are closely related to each other:</p>
						<p class="fragment">
							A symmetric normalized graph Laplacian:
						\[
							L_{sym}:=D^{-1/2}LD^{-1/2}=I-D^{-1/2}WD^{-1/2}
							\]</p>
						<p class="fragment">
							A Random Walk like normalized graph Laplacian:
						\[
							L_{rw}:=D^{-1}L=I-d^{-1}W
							\]</p>
					</section>
					<section id="prop_of_L_and_L_0">
						<h3>The normalized graph Laplacians</h3>
						<h4>Properties of $L_{sym}$ and $L_{rw}$</h4>
						<ol>
							<li class="fragment">
								For every vector $f \in \mathbb{R}^n$ we have:
								\[
								f'L_{sym}f=\frac{1}{2}\sum_{i,j=1}^nw_{ij}\Bigg(\frac{f_i}{\sqrt{d_i}}-\frac{f_j}{\sqrt{d_j}}\Bigg)^2
								\]
							</li>
							<li class="fragment">
								$\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $\lambda$ is an eigenvalue of $L_{sym}$ with eigenvector $w=D^{1/2}u$.
							</li>
							<li class="fragment">
								$\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $\lambda$ and $u$ solve the generalized eigen-problem $Lu=\lambda Du$.
							</li>
						</ol>
					</section>
					<section id="prop_of_L_and_L">
						<h3>The normalized graph Laplacians</h3>
						<h4>Properties of $L_{sym}$ and $L_{rw}$ - continue</h4>
						<ol start="4">
							<li class="fragment">
								$0$ is an eignevalue of $L_{rw}$ with the constant one vector $\mathbb{1}$ as eigenvector. $0$ is an eigenvalue of $L_{sym}$ with eigenvector $D^{1/2}\mathbb{1}$.
							</li>
							<li class="fragment">
								$L_{sym}$ and $L_{rw}$ are positive semi-definite and have $n$ non-negative real-valued eigenvalues $0=\lambda_1\leq\lambda_2\leq\cdot\cdot\cdot\leq\lambda_n$.
							</li>
						</ol>
						<br />
						<br />
						<small class="fragment">The <i>proof</i> of the 5 properties can be found in the appendix <a href="#proof3">here</a>.</small>
					</section>
					<section id="spectrum_of_L_and_L">
						<h3>The normalized Graph Laplacians</h3>
						<h4>Number of connected components, the spectrum of $L_{sym}$ & $L_{rw}$</h4>
						<p>Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue $0$ of both $L_{rw}$ and $L_{sym}$ equals the number of connected components $A_1,...,A_k$ in the graph. <br />For $L_{rw}$, the eigenspace of $0$ is spanned by the indicator vectors $\mathbb{1}_{A_1},...,\mathbb{1}_{A_k}$ of those components. <br />For $L_{sym}$, the eigenspace of $0$ is spanned by the vectors $D^{1/2}\mathbb{1}_{A_i}$.</p>
						<br />
						<br />
						<small class="fragment">The <i>proof</i> can be found in the appendix <a href="#proof4">here</a>.</small>
					</section>
				</section>


				<section>
					<section>
						<h3>Appendices</h3>
					</section>
					<section id="proof1">
						<h3>Proof of the unnormalized graph Laplacian Properties</h3>
						<a href="#unnormalized_lapl">back</a>
					</section>
					<section id="proof2">
						<h3>Proof of the spectrum of L</h3>
						<a href="#spectrum_of_L">back</a>
					</section>
					<section id="proof3">
						<h3>Proof of the properties of L and L</h3>
						<a href="#prop_of_L_and_L">back</a>
					</section>
					<section id="proof4">
						<h3>Proof of the spectrum of L and L</h3>
						<a href="#spectrum_of_L_and_L">back</a>
					</section>
				</section>
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// More info https://github.com/ilanp13/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				math: {
					mathjax: 'plugin/MathJax/MathJax.js',
					config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				},
				// More info https://github.com/ilanp13/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
